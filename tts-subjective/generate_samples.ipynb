{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a380bc6-07ab-41ed-b5d9-a0de976af619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../tts/\")\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from datasets import LJSPEECHList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2950ea5f-f776-4808-8633-5f868c391a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchaudio.datasets.ljspeech.LJSPEECH at 0x7ff8c06fb430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset if you haven't\n",
    "# torchaudio.datasets.LJSPEECH(\"../tts/\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9b5a4-2da0-4951-9590-7602f220fcd4",
   "metadata": {},
   "source": [
    "# torchaudio implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f3df464-0a45-459d-9641-dad8f5c6414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models import Tacotron2, WaveRNN\n",
    "sys.path.append(\"../tts/wavernn/\")\n",
    "from processing import NormalizeDB\n",
    "from wavernn_inference_wrapper import WaveRNNInferenceWrapper\n",
    "from text.text_preprocessing import (\n",
    "    text_to_sequence,\n",
    ")\n",
    "\n",
    "class NormalizeDB(torch.nn.Module):\n",
    "    r\"\"\"Normalize the spectrogram with a minimum db value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_level_db, normalization):\n",
    "        super().__init__()\n",
    "        self.min_level_db = min_level_db\n",
    "        self.normalization = normalization\n",
    "\n",
    "    def forward(self, specgram):\n",
    "        specgram = torch.log10(torch.clamp(specgram.squeeze(0), min=1e-5))\n",
    "        if self.normalization:\n",
    "            return torch.clamp(\n",
    "                (self.min_level_db - 20 * specgram) / self.min_level_db, min=0, max=1\n",
    "            )\n",
    "        return specgram\n",
    "\n",
    "# inverse of the normalization done when training Tacotron2\n",
    "# needed for WaveRNN and Griffin-Lim as WaveGlow also does the same\n",
    "# normalization\n",
    "class InverseSpectralNormalization(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return torch.exp(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ae6001b-53e6-434b-bf8e-7cc69aabf0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f876fc811f3c45edae9ce7a31d52796a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'mel_specgram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_793100/2483921016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mNormalizeDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_level_db\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmel_specgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_specgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mel_specgram' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "res = torch.load(\"./models/torchaudio_tacotron2_ckpt.pth\")\n",
    "tacotron2 = Tacotron2(n_symbol=38).eval().to(device)\n",
    "tacotron2.load_state_dict({k.replace(\"module.\", \"\"): v for k, v, in res['state_dict'].items()})\n",
    "\n",
    "res = torch.load(\"./models/parallel_wavernn_nvidia_ckpt.pt\")\n",
    "wavernn_model = WaveRNN(upsample_scales=[5, 5, 11], n_classes=2**8, hop_length=275, n_freq=80)\n",
    "wavernn_model.load_state_dict({k.replace(\"module.\", \"\"): v for k, v, in res['state_dict'].items()})\n",
    "wavernn_inference_model = WaveRNNInferenceWrapper(wavernn_model).eval().to(device)\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    InverseSpectralNormalization(),\n",
    "    NormalizeDB(min_level_db=-100, normalization=True),\n",
    ")\n",
    "\n",
    "val_dset = LJSPEECHList(root=\"../tts/\", metadata_path=\"../tts/data/ljs_audio_text_test_filelist.txt\")\n",
    "index = np.random.RandomState(0).choice(np.arange(len(val_dset)), replace=False, size=100)\n",
    "\n",
    "for sample_no, i in tqdm(enumerate(index), total=len(index)):\n",
    "    (waveform, sample_rate, text, _) = val_dset[i]\n",
    "    torchaudio.save(filepath=f\"./audio_samples/original/original_{sample_no:04d}.wav\", src=waveform, sample_rate=sample_rate)\n",
    "    sequence = text_to_sequence(text)\n",
    "    lengths = torch.LongTensor([len(sequence)])\n",
    "    sequences = torch.LongTensor(sequence[:]).reshape(1, -1)\n",
    "    with torch.no_grad():\n",
    "        mel, _, _ = tacotron2.infer(sequences.to(device), lengths.to(device))\n",
    "        mel = transforms(mel)\n",
    "        audio = wavernn_inference_model(mel, mulaw=True, batched=False).cpu()\n",
    "    audio_numpy = audio[0]\n",
    "    torchaudio.save(filepath=f\"./audio_samples/torchaudio/torchaudio_{sample_no:04d}.wav\", src=audio, sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d5cd6-3c09-482e-bdd9-6c765f715d45",
   "metadata": {},
   "source": [
    "# Nvidia's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15413dd-f33b-4095-8478-b3053a5b4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/arbiter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Using cache found in /home/arbiter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "/home/arbiter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub/PyTorch/SpeechSynthesis/Tacotron2/waveglow/model.py:55: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
      "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
      "Q, R = torch.qr(A, some)\n",
      "should be replaced with\n",
      "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1940.)\n",
      "  W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
      "Using cache found in /home/arbiter/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "tacotron2 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tacotron2', model_math='fp16')\n",
    "tacotron2 = tacotron2.to('cuda')\n",
    "tacotron2.eval()\n",
    "\n",
    "waveglow = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_waveglow', model_math='fp16')\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.to('cuda')\n",
    "waveglow.eval()\n",
    "\n",
    "utils = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_tts_utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0bd0fe-7a58-4233-a796-72796eba138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dset = LJSPEECHList(root=\"../tts/\", metadata_path=\"../tts/data/ljs_audio_text_test_filelist.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc9cfa61-096a-4f50-a055-8eae9e09b73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517b53ddbb284083accee9fd1b24b09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = np.random.RandomState(0).choice(np.arange(len(val_dset)), replace=False, size=100)\n",
    "\n",
    "for sample_no, i in tqdm(enumerate(index), total=len(index)):\n",
    "    (waveform, sample_rate, text, _) = val_dset[i]\n",
    "    sequences, lengths = utils.prepare_input_sequence([text])\n",
    "    with torch.no_grad():\n",
    "        mel, _, _ = tacotron2.infer(sequences, lengths)\n",
    "        audio = waveglow.infer(mel).cpu()\n",
    "    audio_numpy = audio[0]\n",
    "    torchaudio.save(filepath=f\"./audio_samples/nvidia/nvidia_{sample_no:04d}.wav\", src=audio, sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5409d4b6-6257-4348-a6c8-50b3dc6e407e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_no"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
